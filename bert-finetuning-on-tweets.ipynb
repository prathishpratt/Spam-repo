{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\n\n#With TFAutomodel, we can pull any model from huggingface.\nfrom transformers import TFAutoModel\n\n#To tokenize the tweets\nfrom transformers import AutoTokenizer\n\n#Datasets are loaded from a dataset loading script that downloads and generates the dataset.\n#From HuggingFace\nfrom datasets import load_dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we will pull the model\n#I'm using the BERT_Base_Uncased here\n\n#BERT -Bert model\n#bert-base-uncased ==> params: 110M \n\n#Base - the base version of the model\n# The BERT base has 12 transformer encoder layers stacked while the \n# BERT large has 24 transformer encoder layers.\n\n#Uncased - meaning, the model doesnt distinguish b/w upper and\n# lower case text.\n\nmodel = TFAutoModel.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We need to tokenize the entire text as the output from the \n#Tokenizer is used as the input to the BERT model,\n\n#Example Cases:\n\nexp = tokenizer([\"Stat Models\",\"Stat Thinking and Design\"],\n                padding=True, return_tensors='tf')\nexp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The 102 and 102 indicate the beginning and end of an input sequence\n#These are represented by the string values [CLS] and [SEP], \n#and are inserted automatically into the tokenizer output.\n\n#The 28093 and 4275 are the values for Stat and Models\n\n#Padding is done to keep the input size as constants, in above it is \n# 6 with cls and sep tokens\n\n#Truncation - True truncates any text that is more than 512 words,\n#ehich is the maximum context size for BERT, it the max amount of \n#workds a model can accept.\n\n#return_tensors=True converts the usual list returned to tensors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = model(exp)\noutput","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It gives 2 outputs, the last hidden state(ie the encoder representation) and the pooler output.\n\nshape=(2, 6, 768), is the last hidden states shape.\nThe 2 is the number of input sentence we gave.\nThe 6 is the padded size of the tokens.\n768 is the hidden size of the bert base model. BERT large - 1024.\n\nHidden size is the total number of neurons in the feed forward layer of the encoder.\n\nPooler output size - (2, 768). In pooler output, you wont have the mioddle dimention. For cases like text classification, you dont need the hidden state for every word We just want a single hidden state for each sentence. So each sentence will have a hidden state in pooler output.","metadata":{}},{"cell_type":"markdown","source":"The encoder will output a fixed size vector called **encoder representation.** This vector will have the entire summary of every word in the input sequence. \n\nThe bidirectional attention of the BERT enables it to context both previous and future tokens wile generating output.","metadata":{}},{"cell_type":"code","source":"# Load the dataset\n#!pip install -U datasets\nemotions = load_dataset(\"SetFit/emotion\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotions = load_dataset(\"SetFit/emotion\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotions['train']['text'][0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenize the text","metadata":{}},{"cell_type":"code","source":"#Tokenize function\ndef tokenize(batch):\n    return tokenizer(batch['text'],\n                padding=True, return_tensors='tf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Map or apply the function to all the values of the dataset\n'''\nemotions.map will take all the train,test and valid text\n\nThe map() function supports working with batches of examples. \n\nOperate on batches by setting batched=True. The default batch size \nis 1000, but you can adjust it with the batch_size parameter. \nBatch processing enables interesting applications such as \nsplitting long sentences into shorter chunks and data augmentation.\n'''\n\nemotions_encoded = emotions.map(tokenize, batched= True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotions_encoded['train']['input_ids'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(emotions_encoded['train']['input_ids'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Meaning it is padded with 70 as the size of the sentence.\n\nNow we need to convert from huggingface dataset format to tensorflow datasets format","metadata":{}},{"cell_type":"code","source":"#FOrked from https://www.kaggle.com/code/pritishmishra/fine-tune-bert-for-text-classification?scriptVersionId=116951029\n\n# setting 'input_ids', 'attention_mask', 'token_type_ids', and 'label'\n# to the tensorflow format. Now if you access this dataset you will get these\n# columns in `tf.Tensor` format\n\nemotions_encoded.set_format('tf', \n                            columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n\n# setting BATCH_SIZE to 64.\nBATCH_SIZE = 64\n\ndef order(inp):\n    '''\n    This function will group all the inputs of BERT\n    into a single dictionary and then output it with\n    labels.\n    '''\n    data = list(inp.values())\n    return {\n        'input_ids': data[1],\n        'attention_mask': data[2],\n        'token_type_ids': data[3]\n    }, data[0]\n\n# converting train split of `emotions_encoded` to tensorflow format\ntrain_dataset = tf.data.Dataset.from_tensor_slices(emotions_encoded['train'][:])\n# set batch_size and shuffle\ntrain_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\n# map the `order` function\ntrain_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n\n# ... doing the same for test set ...\ntest_dataset = tf.data.Dataset.from_tensor_slices(emotions_encoded['test'][:])\ntest_dataset = test_dataset.batch(BATCH_SIZE)\ntest_dataset = test_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp, out = next(iter(train_dataset)) # a batch from train_dataset\nprint(inp, '\\n\\n', out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cretate a class for the model \n#Inherting the tf.keras.Model as the input\n\nclass BERTforClassification(tf.keras.Model):\n    \n    def __init__(self,bert_model, num_classes):\n        #Calling super.innt which will call the innt of the \n        #parent class to initialize the model\n        \n        super().__init__()\n        \n        #store the bertmodel\n        self.bert = bert_model\n        \n        #crete a dense layer with num_classes units\n        self.fc = tf.keras.layers.Dense(num_classes,\n                                       activation='softmax')\n        #softmax as we want the prob dist of which class\n        \n        \n    #Write the forward pass in the call method\n    def call(self, inputs):\n        #The input is the tokenized text\n        \n        #For text calssification we need the pooler output, so slice it\n        x = self.bert(inputs)[1]\n        \n        #pass this outout of bert to a dense layer\n        return self.fc(x)\n        \n#This is the whole model        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create an instance of the model\nclassifier = BERTforClassification(model, num_classes = 6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Complile the model\nclassifier.compile(\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\nloss = tf.keras.losses.SparseCategoricalCrossentropy(),\nmetrics = ['accuracy']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#STart the training\nhistory = classifier.fit(train_dataset, epochs = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}